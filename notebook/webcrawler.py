# -*- coding: utf-8 -*-
"""WebCrawler.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J11KWSbdMn49vQysLNB7ouFIG9GvJALe

note
1. 需要安装bs4和pandas
2. 代码里面有中文
3. 可以完整运行所有dataset，运行datasetEN3时需要取消第34行的注释
"""

# -*- coding = utf-8 -*-
import csv
import re
import os
import urllib.request
import urllib.error
from bs4 import BeautifulSoup
from csv import reader
import pandas as pd

# RegEx
title_location = "出版地"
title_time = "出版年份"
title_publisher = "出版者"
title_subject = "主題"

extract_others = re.compile(r'dir="ltr">(.*?)</span>')
extract_subject = re.compile(r'theme=WEB">(.*?)</a>')

# filename: the original data
datafilename = "Dataset_EN.csv"
# filename: raw data
rawdatafilename = "rawdata.csv"
# filename: processed raw data
processedname = "processed.csv"
# filename: output
outputname = "Dataset_EN_Merged.csv"
encoding_read = 'utf-8-sig'
encoding_write = 'utf-8-sig'
alt_encoding = 'gbk'


# uncomment this line for Dataset_EN3.csv
# encoding_read = alt_encoding


def main():
    # listurl(datafilename)
    # getData()
    processcsv()
    # os.remove(rawdatafilename)
    # os.remove(processedname)


def listurl(filename):  # a function that returns a URL list
    global urllist
    with open(filename, "r", encoding=encoding_read) as f:
        f_reader = reader(f)
        urllist = []
        for row in f_reader:
            if len(row[7]) != 0:
                urllist.append(row[7])

    del urllist[0]
    return urllist


# get data from website
def getData():  # get data from website
    datalist = []
    f = open(rawdatafilename, 'w', encoding=encoding_write, newline='')
    writer = csv.writer(f)

    # printed to show work progress
    counter = 0
    for each in urllist:
        url = each
        html = askURL(url)
        counter += 1

        # parse html
        soup = BeautifulSoup(html, "html.parser")

        # print counter to indicate counter% work done
        print(counter)

        # list of data get from website
        csvrow = []
        csvrow.append(url)
        # "Language" manually input = "English"
        for x in range(len(soup.select("td.label"))):
            # append location to row
            if soup.select("td.label")[x].get_text() == title_location:
                location_in_string = str(soup.select("td.label + td")[x])
                csvrow.append(re.findall(extract_others, location_in_string)[0])

        if len(csvrow) < 2:
            print("No Location found")
            csvrow.append(" ")

        for x in range(len(soup.select("td.label"))):
            # append publisher to row
            if soup.select("td.label")[x].get_text() == title_publisher:
                publisher_in_string = str(soup.select("td.label + td")[x])
                csvrow.append(re.findall(extract_others, publisher_in_string)[0])

        if len(csvrow) < 3:
            print("No Publisher found")
            csvrow.append(" ")

        for x in range(len(soup.select("td.label"))):
            # append time to row
            if soup.select("td.label")[x].get_text() == title_time:
                time_in_string = str(soup.select("td.label + td")[x])
                csvrow.append(re.findall(extract_others, time_in_string)[0])

        if len(csvrow) < 4:
            print("No time found")
            csvrow.append(" ")

        # append English to row, row length = 5
        csvrow.append("English")

        for x in range(len(soup.select("td.label"))):
            # append subject to row
            if soup.select("td.label")[x].get_text() == "主題":
                subject_in_string = str(soup.select("td.label + td")[x])
                # string used to store subjects
                list_of_subject = re.findall(extract_subject, subject_in_string)
                tempstr = ""
                for eachitem in range(len(list_of_subject)):
                    tempstr += list_of_subject[eachitem]
                    if eachitem != len(list_of_subject) - 1:
                        tempstr += "; "
                csvrow.append(tempstr)
        if len(csvrow) < 6:
            print("There is no subject for this book!")

        writer.writerow(csvrow)
        datalist.append(html)

    f.close()
    return datalist


# get content from url
def askURL(url):
    head = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472"
                      ".114 Safari/537.36"
    }

    request = urllib.request.Request(url=url, headers=head)
    html = ""
    try:
        response = urllib.request.urlopen(request)
        html = response.read().decode("utf-8")
    except urllib.error.URLError as e:
        if hasattr(e, "code"):
            print(e.code)
        if hasattr(e, "reason"):
            print(e.reason)
    # print(html)
    return html


def processcsv():
    colnames = ['URL in HKPL online catalogue', 'Place of Publication', 'Publisher', 'Year', 'Language', 'Subject']
    rows = []

    # reading csv file
    with open(rawdatafilename, 'r', encoding=encoding_write) as csvfile:
        # creating a csv reader object
        csvreader = csv.reader(csvfile)

        # extracting each data row one by one
        for row in csvreader:
            # remove other characters in row1, "[" and "]"
            row[1] = re.sub(r":", "", row[1])
            row[1] = re.sub(r"\[", "", row[1])
            row[1] = re.sub(r"\]", "", row[1])
            # remove whitespace
            row[1] = row[1].strip()

            # remove comma
            length = len(row[2])
            row[2] = re.sub(r",$", "", row[2])

            # year: format yyyy
            try:
                row[3] = re.findall(r"\d{4}", row[3])[0]
            except:
                print("There is one missing value")
                row[3] = "YYYY"

            newrow = []
            for elements in row:
                # remove dots
                elements = re.sub(r"\.;", ";", elements)
                # remove ; at the end
                elements = re.sub(r"; $", "", elements)
                elements = re.sub(r"\.$", "", elements)
                newrow.append(elements)
            rows.append(newrow)
    csvfile.close()

    # write reformatted rows into new csv file
    f = open(processedname, 'w', encoding=encoding_write, newline='')
    writer = csv.writer(f)
    writer.writerow(colnames)
    for row in rows:
        writer.writerow(row)
    f.close()

    # merging CSV
    a = pd.read_csv(datafilename, encoding=encoding_read)
    b = pd.read_csv(processedname, encoding=encoding_write)
    merged = a.merge(b, on='URL in HKPL online catalogue')
    merged.to_csv(outputname, encoding=encoding_write, index=False)

    # print
    print("Merge successful!")


if __name__ == "__main__":  # execute
    main()